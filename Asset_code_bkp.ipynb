{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8HNMbPnvoaW",
        "outputId": "fcb067bd-9837-4c51-8268-5efe081d7982"
      },
      "source": [
        "!pip install tabula-py\n",
        "!pip install pdf2image\n",
        "!pip install tensorflow==2.0.0\n",
        "!pip uninstall h5py\n",
        "!pip install 'h5py==2.10.0'\n",
        "!apt-get install poppler-utils\n",
        "!pip install PyPDF2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tabula-py\n",
            "  Downloading tabula_py-2.3.0-py3-none-any.whl (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 95 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from tabula-py) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tabula-py) (1.19.5)\n",
            "Collecting distro\n",
            "  Downloading distro-1.6.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.3->tabula-py) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.3->tabula-py) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25.3->tabula-py) (1.15.0)\n",
            "Installing collected packages: distro, tabula-py\n",
            "Successfully installed distro-1.6.0 tabula-py-2.3.0\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.16.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from pdf2image) (7.1.2)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.16.0\n",
            "Collecting tensorflow==2.0.0\n",
            "  Downloading tensorflow-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (86.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3 MB 20 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.12.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.2.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.3 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "  Downloading tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449 kB)\n",
            "\u001b[K     |████████████████████████████████| 449 kB 36.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (3.17.3)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "  Downloading tensorboard-2.0.2-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 39.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.37.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (3.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.41.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.3.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.35.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==2.0.0) (1.5.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.6.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=a2f8a148e4ea498a1a6615508558e6e104cba1fdd8fd73eaac3d8a444c7abe0e\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.6.0\n",
            "    Uninstalling tensorflow-estimator-2.6.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.6.0\n",
            "    Uninstalling tensorboard-2.6.0:\n",
            "      Successfully uninstalled tensorboard-2.6.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.6.0\n",
            "    Uninstalling tensorflow-2.6.0:\n",
            "      Successfully uninstalled tensorflow-2.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.14.1 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n",
            "Found existing installation: h5py 3.1.0\n",
            "Uninstalling h5py-3.1.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/h5py-3.1.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/h5py.libs/libaec-9c9e97eb.so.0.0.10\n",
            "    /usr/local/lib/python3.7/dist-packages/h5py.libs/libhdf5-00e8fae8.so.200.0.0\n",
            "    /usr/local/lib/python3.7/dist-packages/h5py.libs/libhdf5_hl-383c339f.so.200.0.0\n",
            "    /usr/local/lib/python3.7/dist-packages/h5py.libs/libsz-e7aa62f5.so.2.0.1\n",
            "    /usr/local/lib/python3.7/dist-packages/h5py.libs/libz-eb09ad1d.so.1.2.3\n",
            "    /usr/local/lib/python3.7/dist-packages/h5py/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled h5py-3.1.0\n",
            "Collecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 14.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.19.5)\n",
            "Installing collected packages: h5py\n",
            "Successfully installed h5py-2.10.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 154 kB of archives.\n",
            "After this operation, 613 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 poppler-utils amd64 0.62.0-2ubuntu2.12 [154 kB]\n",
            "Fetched 154 kB in 0s (1,898 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 155047 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_0.62.0-2ubuntu2.12_amd64.deb ...\n",
            "Unpacking poppler-utils (0.62.0-2ubuntu2.12) ...\n",
            "Setting up poppler-utils (0.62.0-2ubuntu2.12) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting PyPDF2\n",
            "  Downloading PyPDF2-1.26.0.tar.gz (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 4.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyPDF2\n",
            "  Building wheel for PyPDF2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyPDF2: filename=PyPDF2-1.26.0-py3-none-any.whl size=61101 sha256=3fc453286851ad966531057cbd00bfdb7eff9110668dd2c1142a42ef621f3bf7\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1a/24/648467ade3a77ed20f35cfd2badd32134e96dd25ca811e64b3\n",
            "Successfully built PyPDF2\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-1.26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEQWsgDKRiQV"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "\n",
        "# Store Pdf with convert_from_path function\n",
        "for pdf in glob.glob('C:\\ASSET/PDF/*.pdf'):\n",
        "    images = convert_from_path(pdf,poppler_path = r'C:\\ASSET\\poppler-21.09.0\\Library\\bin')\n",
        "    #print(images)\n",
        "    images[0].save(str(pdf)[:-4]+'.jpg', 'JPEG')\n",
        "\t\n",
        "\n",
        "# XML to CSV Conversion\n",
        "def xml_to_csv(path):\n",
        "xml_list = []\n",
        "for xml_file in glob.glob(path+'/*.xml'):\n",
        "tree = ET.parse(xml_file)\n",
        "root = tree.getroot()\n",
        "for member in root.findall('object'):\n",
        "value = (root.find('filename').text,int(member[4][0].text),int(member[4][1].text),int(member[4][2].text),int(member[4][3].text),member[0].text)\n",
        "xml_list.append(value)\n",
        "        column_name = ['filename','xmin','ymin','xmax','ymax','class']\n",
        "        xml_df = pd.DataFrame(xml_list,columns = column_name)\n",
        "    return xml_df\n",
        "   \n",
        "def main():\n",
        "    image_path = os.path.join(os.getcwd(),'C:\\ASSET\\')\n",
        "    print(image_path)\n",
        "    xml_df = xml_to_csv(image_path)\n",
        "    xml_df.to_csv('new199.csv',index=None)\n",
        "    print('successfully converted xml to csv.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1Gz_hZERspr"
      },
      "source": [
        "#Training the model using the bounding box coordinates\n",
        "import tensorflow as tf\n",
        "#from src import config\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Flatten,Dense,Dropout,Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import pickle\n",
        "from cv2 import cv2\n",
        "import os\n",
        "\n",
        "#load the contents of annotated csv file\n",
        "print(\"[INFO] loading dataset ..\")\n",
        "data = []\n",
        "labels = []\n",
        "targets = []\n",
        "imagePaths = []\n",
        "\n",
        "#for csvPath in paths.list_files(config.ANNOTS_PATH,validExts=(\".csv\")):\n",
        "    #rows = open(csvPath).read().strip().split(\"\\n\")\n",
        "\n",
        "rows = open('new.csv').read().strip().split(\"\\n\")\n",
        "\n",
        "k=0\n",
        "for row in rows:\n",
        "    # break the row into the filename, bounding box coordinates,\n",
        "    # and class label\n",
        "    row = row.split(\",\")\n",
        "    (filename, startX, startY, endX, endY, label) = row\n",
        "    # derive the path to the input image, load the image (in OpenCV\n",
        "    # format), and grab its dimensions\n",
        "    #    imagepath = os.path.sep.join([config.IMAGES_PATH,label,filename])\n",
        "    imagepath = \"C:\\\\ASSET\\\\PDF\\\\\" + filename\n",
        "    print(\"imagepath\",imagepath)\n",
        "    image = cv2.imread(imagepath)\n",
        "    k = k+1\n",
        "    print(\"FOUND AND READ IMAGE: \",imagepath,\"with k:\",k)\n",
        "    crop_image = image[int(startY): int(endY),int(startX): int(endX)]\n",
        "    \n",
        "    (h, w) = image.shape[:2]\n",
        "    # scale the bounding box coordinates relative to the spatial dimensions of the input image\n",
        "    startX = float(startX) / w\n",
        "    startY = float(startY) / h\n",
        "    endX = float(endX) / w\n",
        "    endY = float(endY) / h\n",
        "       \n",
        "    # load the image and preprocess it\n",
        "    image = load_img(imagepath, target_size=(224, 224))\n",
        "    image = img_to_array(image)\n",
        "    # update our list of data, class labels, bounding boxes, and\n",
        "    # image paths\n",
        "    data.append(image)\n",
        "    labels.append(label)\n",
        "    targets.append((startX, startY, endX, endY))\n",
        "    imagePaths.append(imagepath)\n",
        "       \n",
        "# convert the data, class labels, bounding boxes, and image paths to\n",
        "# NumPy arrays, scaling the input pixel intensities from the range\n",
        "# [0, 255] to [0, 1]\n",
        "data = np.array(data, dtype=\"float32\") / 255.0\n",
        "labels = np.array(labels)\n",
        "targets = np.array(targets, dtype=\"float32\")\n",
        "imagePaths = np.array(imagePaths)\n",
        "# perform one-hot encoding on the labels\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "# only there are only two labels in the dataset, then we need to use\n",
        "# Keras/TensorFlow's utility function as well\n",
        "if len(lb.classes_) == 2:\n",
        "    labels = to_categorical(labels)\n",
        "    \n",
        "# partition the data into training and testing splits using 80% of\n",
        "# the data for training and the remaining 20% for testing\n",
        "split = train_test_split(data, labels, targets, imagePaths,test_size=0.20, random_state=42)\n",
        "\n",
        "# unpack the data split\n",
        "(trainImages, testImages) = split[:2]\n",
        "(trainLabels, testLabels) = split[2:4]\n",
        "(trainBBoxes, testBBoxes) = split[4:6]\n",
        "(trainPaths, testPaths) = split[6:]\n",
        "# write the testing image paths to disk so that we can use then\n",
        "# when evaluating/testing our object detector\n",
        "print(\"[INFO] saving testing image paths...\")\n",
        "TEST_PATHS = \"C:\\\\ASSET\\\\PDF\\\\test_images.txt\"\n",
        "f = open(TEST_PATHS, \"w\")\n",
        "f.write(\"\\n\".join(testPaths))\n",
        "f.close()\n",
        "\n",
        "# load the VGG16 network, ensuring the head FC layers are left off\n",
        "vgg = VGG16(weights=\"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\", include_top=False,\n",
        "    input_tensor=Input(shape=(224, 224, 3)))\n",
        "# freeze all VGG layers so they will *not* be updated during the\n",
        "# training process\n",
        "vgg.trainable = False\n",
        "# flatten the max-pooling output of VGG\n",
        "flatten = vgg.output\n",
        "flatten = Flatten()(flatten)\n",
        "\n",
        "# construct a fully-connected layer header to output the predicted\n",
        "# bounding box coordinates\n",
        "bboxHead = Dense(256, activation=\"relu\")(flatten)\n",
        "bboxHead = Dropout(0.2)(bboxHead)\n",
        "bboxHead = Dense(128, activation=\"relu\")(bboxHead)\n",
        "bboxHead = Dropout(0.2)(bboxHead)\n",
        "bboxHead = Dense(64, activation=\"relu\")(bboxHead)\n",
        "bboxHead = Dropout(0.2)(bboxHead)\n",
        "bboxHead = Dense(32, activation=\"relu\")(bboxHead)\n",
        "bboxHead = Dropout(0.2)(bboxHead)\n",
        "bboxHead = Dense(4, activation=\"sigmoid\",name=\"bounding_box\")(bboxHead)\n",
        "# construct a second fully-connected layer head, this one to predict the class label\n",
        "#softmaxHead = Dense(512, activation=\"relu\")(flatten)\n",
        "#softmaxHead = Dropout(0.5)(softmaxHead)\n",
        "#softmaxHead = Dense(512, activation=\"relu\")(softmaxHead)\n",
        "#softmaxHead = Dropout(0.5)(softmaxHead)\n",
        "#softmaxHead = Dense(len(lb.classes_), activation=\"softmax\",name=\"class_label\")(softmaxHead)\n",
        "\n",
        "# construct the model we will fine tune for bbox regression\n",
        "model = Model(inputs=vgg.input,outputs=(bboxHead))\n",
        "\n",
        "losses = {\"bounding_box\":\"mean_squared_error\"}\n",
        "\n",
        "lossWeights = {\"bounding_box\": 1.0}\n",
        "\n",
        "NUM_EPOCHS = 100\n",
        "INIT_LR = 1e-4\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# initialize the optimizer, compile the model, and show the model summary\n",
        "opt = Adam(lr=INIT_LR)\n",
        "model.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=lossWeights)\n",
        "print(model.summary())\n",
        "\n",
        "# construct a dictionary for our target training outputs\n",
        "trainTargets = {\n",
        "    \"bounding_box\": trainBBoxes\n",
        "}\n",
        "# construct a second dictionary, this one for our target testing\n",
        "# outputs\n",
        "testTargets = {\n",
        "    \"bounding_box\": testBBoxes\n",
        "}\n",
        "\n",
        "# train the network for bounding box regression\n",
        "print(\"[INFO] training bounding box regression...\")\n",
        "checkpoint_path = \"training_4/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "model.save_weights(checkpoint_path.format(epoch=0))\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path, \n",
        "    verbose=1, \n",
        "    save_weights_only=True,\n",
        "    save_freq=25*BATCH_SIZE)\n",
        "    \n",
        "H = model.fit(\n",
        "    trainImages, trainTargets,\n",
        "    validation_data=(testImages, testTargets),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=NUM_EPOCHS,\n",
        "       callback= [cp_callback],\n",
        "    verbose=1)\n",
        "# serialize the model to disk\n",
        "print(\"[INFO] saving object detector model...\")\n",
        "TABLE_MODEL_PATH = os.path.sep.join([os.getcwd(),\"Table_Detector.h5\"])\n",
        "TABLE_PICKLE = os.path.sep.join([os.getcwd(),\"Table_lb.pickle\"])\n",
        "model.save(TABLE_MODEL_PATH)\n",
        "# serialize the label binarizer to disk\n",
        "f = open(TABLE_PICKLE, \"wb\")\n",
        "f.write(pickle.dumps(lb))\n",
        "f.close()\n",
        "\n",
        "PLOT_PATH = \"C:\\\\ASSET\\\\PDF\\\\\"\n",
        "#BASE_OUTPUT = os.getcwd()\n",
        "BASE_OUTPUT = \"C:\\\\ASSET\\\\PDF\\\\\"\n",
        "# plot the total loss, label loss, and bounding box loss\n",
        "lossNames = [\"loss\"]\n",
        "N = np.arange(0,NUM_EPOCHS)\n",
        "plt.style.use(\"ggplot\")\n",
        "(fig, ax) = plt.subplots(3, 1, figsize=(13, 13))\n",
        "# loop over the loss names\n",
        "for (i, l) in enumerate(lossNames):\n",
        "    # plot the loss for both the training and validation data\n",
        "    title = \"Loss for {}\".format(l) if l != \"loss\" else \"Total loss\"\n",
        "    ax[i].set_title(title)\n",
        "    ax[i].set_xlabel(\"Epoch #\")\n",
        "    ax[i].set_ylabel(\"Loss\")\n",
        "    ax[i].plot(N, H.history[l], label=l)\n",
        "    ax[i].plot(N, H.history[\"val_\" + l], label=\"val_\" + l)\n",
        "    ax[i].legend()\n",
        "# save the losses figure and create a new figure for the accuracies\n",
        "plt.tight_layout()\n",
        "plotPath = os.path.sep.join([BASE_OUTPUT, \"losses.png\"])\n",
        "plt.savefig(plotPath)\n",
        "plt.close()\n",
        "\n",
        "N = NUM_EPOCHS\n",
        "\n",
        "plotPath = os.path.sep.join([BASE_OUTPUT, \"acc.png\"])\n",
        "plt.savefig(plotPath)\n",
        "\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.title(\"Bounding box regression loss on training set\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(PLOT_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "lGZDG_jMMKXO",
        "outputId": "62a0408b-401a-449c-b42d-a013dd7912fe"
      },
      "source": [
        "#Test the model\n",
        "from pdf2image import convert_from_path\n",
        "fn = 'test1.pdf'\n",
        "images = convert_from_path(fn)\n",
        "images[0].save('test.png', 'PNG')\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "import io\n",
        "from PyPDF2 import PdfFileReader \n",
        "from tensorflow.keras.models import load_model\n",
        "from tabula import read_pdf\n",
        "import math\n",
        "from cv2 import cv2\n",
        "with io.open(fn,mode = \"rb\") as f:\n",
        "  input_pdf = PdfFileReader(f)\n",
        "  media_box = input_pdf.getPage(0).mediaBox\n",
        "min_pt = media_box.lowerLeft\n",
        "max_pt = media_box.upperRight\n",
        "image = load_img('test.png',target_size = (224,224))\n",
        "image = img_to_array(image)/255.0\n",
        "image = np.expand_dims(image,axis = 0)\n",
        "\n",
        "borderless_model = load_model(\"/content/drive/MyDrive/ts/Table_Detector_borderless.h5\",compile=False)\n",
        "borderless_preds = borderless_model.predict(image)[0]\n",
        "\n",
        "(borderless_startX,borderless_startY,borderless_endX,borderless_endY) = borderless_preds\n",
        "from cv2 import cv2\n",
        "image = cv2.imread('test.png')\n",
        "\n",
        "image_cp = image.copy()\n",
        "\n",
        "(h,w) = image.shape[:2]\n",
        "\n",
        "borderless_startX = int(borderless_startX*w)\n",
        "borderless_startY = int(borderless_startY*h)\n",
        "borderless_endX = int(borderless_endX*w)\n",
        "borderless_endY = int(borderless_endY*h)\n",
        "\n",
        "#final_pdf_coordinates = str(math.floor(borderless_startY/2)-5)+\",\"+str(math.floor(min_pt[0]))+\",\" + str(math.floor(borderless_endY/2)) + \",\"+ str(math.ceil(max_pt[0]))\n",
        "final_pdf_coordinates = str(math.floor(borderless_startY/2)-20)+\",\"+str(math.floor(borderless_startX/2))+\",\" + str(math.floor(borderless_endY/2)+60) + \",\"+ str(math.floor(borderless_endX/2)-5)\n",
        "\n",
        "\n",
        "print(\"FINAL_TABLE_COORDINATES: \",final_pdf_coordinates)\n",
        "\n",
        "#cv2.rectangle(image,(int(min_pt[0]),borderless_startY,),(int(max_pt[0]*2),borderless_endY),(0,255,0),2)\n",
        "#cv2.rectangle(image,(borderless_startX,borderless_startY-5,),(borderless_endX,borderless_endY),(0,255,0),2)\n",
        "cv2.rectangle(image,(borderless_startX,borderless_startY,),(borderless_endX,borderless_endY),(0,255,0),2)\n",
        "\n",
        "imS = cv2.resize(image,(560,640))\n",
        "\n",
        "tables = read_pdf(fn,area = [final_pdf_coordinates],stream = True,output_format = \"dataframe\",pandas_options = {'header':None})\n",
        "if len(tables) > 0:\n",
        "\tdf = pd.DataFrame(np.concatenate(tables))\n",
        "\t\n",
        "else:\n",
        "  print(\"No table\")\n",
        "  df = pd.DataFrame()\n",
        "\n",
        "cv2.imwrite('op.jpg', imS)\n",
        "df.head()\n",
        "#cv2.imshow(\"model_output\",imS)\n",
        "\n",
        "#cv2.waitkey(0)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "'pages' argument isn't specified.Will extract only from page 1 by default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FINAL_TABLE_COORDINATES:  70,4,150,838\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Category</td>\n",
              "      <td>Projected_Cost</td>\n",
              "      <td>Actual_Cost</td>\n",
              "      <td>Difference</td>\n",
              "      <td>Projected_Cost</td>\n",
              "      <td>Actual_Cost</td>\n",
              "      <td>Difference</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Home</td>\n",
              "      <td>100</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>100</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Health</td>\n",
              "      <td>100</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>100</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Life</td>\n",
              "      <td>100</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>100</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Other</td>\n",
              "      <td>100</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>100</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0               1  ...            5           6\n",
              "0  Category  Projected_Cost  ...  Actual_Cost  Difference\n",
              "1      Home             100  ...           50          50\n",
              "2    Health             100  ...           50          50\n",
              "3      Life             100  ...           50          50\n",
              "4     Other             100  ...           50          50\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "4YoJuOG54GwB",
        "outputId": "b073b137-a322-4f97-d7da-af4ec9c9714b"
      },
      "source": [
        "\n",
        "fn = 'test1.pdf'\n",
        "tables = read_pdf(fn,area = [70,4,150,838],stream = True,output_format = \"dataframe\",pandas_options = {'header':None}, pages=1)\n",
        "df = pd.DataFrame(np.concatenate(tables))\n",
        "df"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Category</td>\n",
              "      <td>Projected_Cost</td>\n",
              "      <td>Actual_Cost</td>\n",
              "      <td>Difference</td>\n",
              "      <td>Projected_Cost</td>\n",
              "      <td>Actual_Cost</td>\n",
              "      <td>Difference</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Home</td>\n",
              "      <td>100</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>100</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Health</td>\n",
              "      <td>100</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>100</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Life</td>\n",
              "      <td>100</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>100</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Other</td>\n",
              "      <td>100</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>100</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0               1  ...            5           6\n",
              "0  Category  Projected_Cost  ...  Actual_Cost  Difference\n",
              "1      Home             100  ...           50          50\n",
              "2    Health             100  ...           50          50\n",
              "3      Life             100  ...           50          50\n",
              "4     Other             100  ...           50          50\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLEL-OSxhh4r"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "h_7VqJf4F29H",
        "outputId": "37523a30-671e-4615-8294-668a1a710cac"
      },
      "source": [
        "#!pip install 'h5py==2.10.0' --force-reinstall\n",
        "#!pip uninstall h5py\n",
        "!pip install 'h5py==2.10.0'\n",
        "#!pwd\n",
        "#/content/test.jpg\n",
        "#/content/sample_data/Table_Detector.h5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting h5py==2.10.0\n",
            "  Using cached h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.21.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.16.0)\n",
            "Installing collected packages: h5py\n",
            "Successfully installed h5py-2.10.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "h5py"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1olY_SHQe1Kj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cace1fd-5fe9-474d-ccc9-66702100a839"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tabula import read_pdf\n",
        "tables = read_pdf('test.pdf',area = [338,200,608,596],stream = True,output_format = \"dataframe\",pandas_options = {'header':None},pages =1)\n",
        "print(tables)\n",
        "# df_concat = pd.DataFrame(np.concatenate(tables))\n",
        "# df_concat.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The output file is empty.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgYl7b4lJ0IN",
        "outputId": "0e7be1d7-04fc-413a-d123-aa1eb5d1abf5"
      },
      "source": [
        "!pip install tabula-py\n",
        "!pip install pdf2image\n",
        "!pip install tensorflow=2.0.0\n",
        "!pip uninstall h5py\n",
        "!pip install 'h5py==2.10.0'\n",
        "!apt-get install poppler-utils\n",
        "!pip install PyPDF2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading PyPDF2-1.26.0.tar.gz (77 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▎                           | 10 kB 23.7 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 61 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 71 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 77 kB 2.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyPDF2\n",
            "  Building wheel for PyPDF2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyPDF2: filename=PyPDF2-1.26.0-py3-none-any.whl size=61101 sha256=2289b89391bfbbefe0a2ed7bdce780260e0f7b6c4150a898ee3d02b7025310ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1a/24/648467ade3a77ed20f35cfd2badd32134e96dd25ca811e64b3\n",
            "Successfully built PyPDF2\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-1.26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySGxVaIpV46L",
        "outputId": "254a4a83-528b-4ef5-f25d-f261591f1d9d"
      },
      "source": [
        "# tables = read_pdf('test.pdf',stream = True,output_format = \"dataframe\",pandas_options = {'header':None})\n",
        "# if len(tables) > 0:\n",
        "# \tdf = pd.DataFrame(np.concatenate(tables))\n",
        "# df.head()\n",
        "#df_concat,currency_code,total_amount = dataframe_processing(df)\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZV6cAOOF9pC"
      },
      "source": [
        "# New Section"
      ]
    }
  ]
}